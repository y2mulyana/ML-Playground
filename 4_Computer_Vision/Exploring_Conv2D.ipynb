{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd105be-3de8-442a-8b35-d27e219e0fba",
   "metadata": {},
   "source": [
    "# Exploring Pytorch nn.Conv2d\n",
    "`nn.Conv2d()`, also known as a convolutional layer. We will do some simulation to see what's going on when we run this `nn.Conv2d()`.\n",
    "\n",
    "Refference:\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "* https://poloclub.github.io/cnn-explainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b3c1d1-6adc-4ce8-8a15-8536aa6cc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4dddbc-06b4-4b73-b3cf-0c015457f4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee4623f5-067b-4e54-865b-53ddc8d98c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
      "Single image shape: torch.Size([3, 64, 64]) -> [color_channels, height, width]\n",
      "Single image pixel values:\n",
      "tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n",
      "         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n",
      "         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n",
      "         ...,\n",
      "         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n",
      "         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n",
      "         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n",
      "\n",
      "        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n",
      "         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n",
      "         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n",
      "         ...,\n",
      "         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n",
      "         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n",
      "         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n",
      "\n",
      "        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n",
      "         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n",
      "         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n",
      "         ...,\n",
      "         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n",
      "         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n",
      "         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# We will create 32 fake images with RGB color and 64x64 pixel size\n",
    "\n",
    "# Create sample batch of random numbers with same size as image batch\n",
    "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
    "\n",
    "# get a single image for testing\n",
    "test_image = images[0] \n",
    "\n",
    "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\") \n",
    "print(f\"Single image pixel values:\\n{test_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a041bd-df33-488b-b4c4-736ac5b0ebe9",
   "metadata": {},
   "source": [
    "## Conv2D 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97bf9e0c-62da-48ce-9ca7-41b973851337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.8778e-01, -6.0596e-02, -5.6306e-02,  ...,  2.8654e-01,\n",
       "           6.6224e-01, -2.3216e-01],\n",
       "         [-9.8911e-01, -4.0099e-01,  4.1832e-01,  ...,  4.7459e-01,\n",
       "          -1.8552e-01, -5.7622e-01],\n",
       "         [-4.1340e-02, -2.3277e-01,  3.7418e-01,  ...,  2.8255e-02,\n",
       "           1.4923e-01,  1.4236e-01],\n",
       "         ...,\n",
       "         [-8.0374e-01, -7.6687e-01, -5.9457e-02,  ...,  1.7452e-01,\n",
       "           4.2594e-01, -4.8341e-01],\n",
       "         [-1.4512e-01, -1.1566e-01,  6.1783e-01,  ...,  2.4126e-01,\n",
       "          -3.6626e-01,  3.5645e-01],\n",
       "         [ 3.6096e-02,  1.5214e-01,  2.3123e-01,  ...,  3.0904e-01,\n",
       "          -4.9680e-01, -7.2258e-01]],\n",
       "\n",
       "        [[-1.0853e+00, -1.6079e+00,  1.3346e-01,  ...,  2.1698e-01,\n",
       "          -1.7643e+00,  2.5263e-01],\n",
       "         [-8.2507e-01,  6.3866e-01,  1.8845e-01,  ..., -1.0936e-01,\n",
       "           4.8068e-01,  8.4869e-01],\n",
       "         [ 6.4927e-01, -4.2061e-03, -4.9991e-01,  ...,  5.8356e-01,\n",
       "           2.4611e-01,  6.6233e-01],\n",
       "         ...,\n",
       "         [ 9.8860e-02,  1.1661e+00,  3.1532e-01,  ..., -6.5450e-01,\n",
       "          -1.9585e-02,  2.4397e-01],\n",
       "         [-5.9820e-01,  3.7339e-01, -7.2705e-01,  ..., -3.1185e-02,\n",
       "          -8.9892e-01,  2.9192e-01],\n",
       "         [-1.8412e-01,  2.2085e-01,  1.6990e-01,  ...,  4.7418e-01,\n",
       "          -9.0971e-01,  7.9518e-01]],\n",
       "\n",
       "        [[ 3.9605e-02,  6.7841e-01, -9.6515e-01,  ..., -2.8578e-01,\n",
       "          -2.9320e-01, -1.8889e-01],\n",
       "         [-1.5027e+00, -1.1401e+00, -2.0217e-01,  ...,  2.5681e-01,\n",
       "          -9.3105e-01,  6.8724e-02],\n",
       "         [-4.1801e-01,  2.6299e-01,  3.3114e-02,  ...,  2.7162e-01,\n",
       "          -6.0643e-01, -7.4065e-01],\n",
       "         ...,\n",
       "         [ 5.3659e-02, -1.6118e-01, -2.2006e-01,  ...,  4.0253e-01,\n",
       "           2.1769e-01,  2.0759e-01],\n",
       "         [ 8.3423e-02,  2.5296e-01, -4.3232e-01,  ...,  1.1411e+00,\n",
       "           9.1973e-01,  3.2513e-02],\n",
       "         [ 2.0770e-01,  6.7494e-01,  1.7256e-01,  ..., -4.1328e-01,\n",
       "          -1.6368e-01, -1.3886e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.5893e-01, -1.0476e+00,  1.0067e+00,  ..., -1.7439e-01,\n",
       "           3.5072e-01,  2.5077e-01],\n",
       "         [ 1.3226e+00,  1.9803e+00, -6.4183e-01,  ...,  3.2681e-01,\n",
       "           1.5939e+00, -1.0821e-01],\n",
       "         [ 8.7122e-01, -1.0276e+00,  4.5702e-01,  ...,  5.1113e-01,\n",
       "           1.0244e+00,  8.7522e-01],\n",
       "         ...,\n",
       "         [ 1.3555e+00,  1.3009e+00,  4.9802e-01,  ..., -1.0216e-01,\n",
       "          -5.6769e-01,  8.4543e-02],\n",
       "         [ 1.5033e-01, -3.4481e-01,  1.0851e+00,  ..., -1.6738e-01,\n",
       "          -5.1884e-01,  1.9113e-01],\n",
       "         [-2.3805e-02, -4.3101e-01,  6.8124e-02,  ...,  1.0441e+00,\n",
       "           4.1791e-01,  6.0961e-01]],\n",
       "\n",
       "        [[-5.7891e-02,  6.3499e-02, -3.7689e-01,  ..., -7.3454e-01,\n",
       "           7.6985e-01, -6.7518e-01],\n",
       "         [ 2.9175e-01,  1.1638e-04, -6.0476e-01,  ..., -1.1983e+00,\n",
       "           6.5636e-01, -5.4662e-01],\n",
       "         [ 2.1523e-01, -1.9851e-01, -1.3285e-01,  ...,  7.5264e-01,\n",
       "          -1.5505e+00,  4.5457e-01],\n",
       "         ...,\n",
       "         [-2.0348e-01, -4.8311e-01, -3.0158e-02,  ..., -5.2867e-01,\n",
       "          -8.0491e-01,  7.7403e-01],\n",
       "         [ 7.5117e-01, -1.5956e-01,  2.5034e-01,  ..., -4.0541e-01,\n",
       "           2.8248e-01, -4.3361e-01],\n",
       "         [-2.1132e-01, -4.4753e-01, -1.0997e-01,  ..., -7.6028e-02,\n",
       "           7.9822e-01, -6.3137e-01]],\n",
       "\n",
       "        [[ 8.6642e-01,  1.6339e+00,  5.4498e-02,  ...,  2.3448e-01,\n",
       "           5.0939e-01,  8.1898e-01],\n",
       "         [ 1.0650e-01,  3.0274e-01, -7.4020e-01,  ...,  8.3167e-01,\n",
       "          -4.6203e-01,  3.4506e-01],\n",
       "         [ 1.1760e-02, -6.1708e-02, -7.6054e-01,  ..., -8.1317e-01,\n",
       "           1.0693e+00, -9.6907e-01],\n",
       "         ...,\n",
       "         [-1.1468e+00, -1.3791e-01, -3.6390e-01,  ...,  5.5073e-01,\n",
       "           2.0598e-01,  1.0710e-01],\n",
       "         [-3.4261e-01, -1.6038e-02, -6.6151e-01,  ..., -4.2419e-01,\n",
       "           4.4527e-01, -1.0111e-01],\n",
       "         [ 4.4487e-02,  9.7506e-02, -3.1829e-01,  ...,  6.0565e-02,\n",
       "           5.3990e-01, -4.2942e-01]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer_1 = nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=10,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=0\n",
    ")\n",
    "\n",
    "conv_layer_1(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5be9e4f-b0e2-44b5-914e-bb5266c71df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ba2ccf-31a3-4bb9-8bfa-daf8cc83de4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 62, 62])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer_1(test_image).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5801b9b-2c61-40c4-a06e-8f8796fe21b5",
   "metadata": {},
   "source": [
    "\n",
    "### Breakdown:\n",
    "\n",
    "The reason the number of channels changes to 10 after applying the convolutional layer is due to the `out_channels` parameter specified when defining the convolutional layer.\n",
    "\n",
    "Here’s a breakdown of what happens:\n",
    "\n",
    "- **Input**: we started with a tensor of shape `[1, 3, 64, 64]`. This means:\n",
    "  - Batch size = 1\n",
    "  - 3 channels (e.g., RGB channels for an image)\n",
    "  - Height and width = 64x64 pixels\n",
    "  \n",
    "- **Convolutional Layer**: We applied a `nn.Conv2d` layer with:\n",
    "  - `in_channels=3`: the layer expects an input with 3 channels (as in the RGB image).\n",
    "  - `out_channels=10`: this means the convolutional layer will produce 10 output channels after the convolution operation. \n",
    "  - `kernel_size=3`: the kernel size is 3x3.\n",
    "  - `stride=1`: the stride is 1, so the kernel moves by 1 pixel at a time.\n",
    "  - `padding=0`: no padding is applied, so the output spatial dimensions shrink.\n",
    "\n",
    "### How the Output Shape is Computed:\n",
    "- The convolution operation applies a filter to the input channels and produces a new set of feature maps, each corresponding to one of the output channels.\n",
    "- Since we set `out_channels=10`, the layer will generate 10 different feature maps. This is why the output has 10 channels.\n",
    "- The spatial dimensions of the feature maps (height and width) are computed as:\n",
    "\n",
    "$\\text{Output Height} = \\frac{\\text{Input Height} - \\text{Kernel Height}}{\\text{Stride}} + 1 = \\frac{64 - 3}{1} + 1 = 62$\n",
    "\n",
    "$\\text{Output Width} = \\frac{\\text{Input Width} - \\text{Kernel Width}}{\\text{Stride}} + 1 = \\frac{64 - 3}{1} + 1 = 62$\n",
    "\n",
    "So, the output shape becomes `[1, 10, 62, 62]`, where:\n",
    "- `1` is the batch size,\n",
    "- `10` is the number of output channels (since we specified `out_channels=10`),\n",
    "- `62` is the new height and width of the output after the convolution.\n",
    "\n",
    "### Exploring??:\n",
    "What's going on if we change kernel to **5**?\n",
    "\n",
    "$\\text{Output Height} = \\frac{\\text{Input Height} - \\text{Kernel Height}}{\\text{Stride}} + 1 = \\frac{64 - 5}{1} + 1 = 60$\n",
    "\n",
    "$\\text{Output Width} = \\frac{\\text{Input Width} - \\text{Kernel Width}}{\\text{Stride}} + 1 = \\frac{64 - 5}{1} + 1 = 60$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f46e9f-95a4-4b9c-8ce8-f7b2dfd5778d",
   "metadata": {},
   "source": [
    "### Try this simple calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2e53e63-51e6-4cf7-8e36-4a0b0a61e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_height: 30\n"
     ]
    }
   ],
   "source": [
    "input_height, input_width = 64 , 64\n",
    "\n",
    "kernel = 5\n",
    "stride = 2\n",
    "\n",
    "output_height = int((input_height - kernel)/ stride) + 1\n",
    "print(f\"output_height: {output_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9e44f-00e2-4c0e-86e4-82f2452397f9",
   "metadata": {},
   "source": [
    "## Conv2D 2\n",
    "\n",
    "We try to change kernel size to 5 and stride = 2, and lets's do with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb7ac0a-05ee-4d4a-a74e-c9e845ac2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_2 = nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=10,\n",
    "    kernel_size=5,\n",
    "    stride=2,\n",
    "    padding=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70581fa9-88f4-4ae2-882a-6bda00fcc23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cffba170-d595-43ab-bb21-b3a2c9841359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 30, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer_2(test_image).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b9a4d-3323-4950-8d5b-ffef828b0242",
   "metadata": {},
   "source": [
    "## Weight & Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f2969e6-a681-4d73-bf37-4e7333c5aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer_2 weight shape: \n",
      "torch.Size([10, 3, 5, 5]) -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n",
      "\n",
      "conv_layer_2 bias shape: \n",
      "torch.Size([10]) -> [out_channels=10]\n"
     ]
    }
   ],
   "source": [
    "# Get shapes of weight and bias tensors within conv_layer_2\n",
    "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
    "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels=10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63177baa-ce68-4ccc-8b52-aef2844bee97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
